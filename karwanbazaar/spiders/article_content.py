"""
Contents of all articles

output format:
{article_id, title, content}

output file format: jsonl
"""

import scrapy
import os
import jsonlines


class ArticleContentScraper(scrapy.Spider):
    name = "article_content"

    def start_requests(self):
        # get article urls from the file generated by article_urls spider
        urls = []

        with open(f"{os.getcwd()}/output/article_urls.txt", "r") as f:
            lines = f.readlines()
            for line in lines:
                urls.append(line.strip())

        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)

    def parse(self, response):
        # article id
        article_id = response.css("div.singlepage div.post::attr(id)").extract()[0]
        # remove the post-* part
        article_id = article_id.replace("post-", "")

        # title
        title = response.css("div.singlepage div.post h2::text").extract()[0]

        # content
        raw_contents = response.css("div.singlepage div.post div.entry p::text").extract()

        # join all raw content strings
        content = " ".join(c.encode("utf-8").decode("utf-8") for c in raw_contents).strip()

        # write to a json lines file
        with jsonlines.open(f"{os.getcwd()}/output/articles.jsonl", "a") as f:
            json_obj = {
                "article_id": article_id,
                "title": title,
                "content": content
            }

            f.write(json_obj)

        self.log(f"Saved article-{article_id}")
