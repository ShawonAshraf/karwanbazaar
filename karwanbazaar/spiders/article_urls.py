"""
Crawls an archive root url
Finds all the article urls
"""

import scrapy
import os


class ArticleURLSpider(scrapy.Spider):
    name = "article_url"

    def start_requests(self):
        # get article urls from the file generated by archives spider
        urls = []

        with open(f"{os.getcwd()}/output/archive_root_urls.txt", "r") as f:
            lines = f.readlines()
            for line in lines:
                urls.append(line.strip())

        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)

    def parse(self, response):
        # get all article urls for a single archive
        article_urls = response.css("div.mainpage div.post h2 a::attr(href)").extract()

        # write to file
        # append, since parse will run for every yield and archive url
        with open(f"{os.getcwd()}/output/article_urls.txt", "a") as f:
            for url in article_urls:
                f.write(url + "\n")

        # log
        self.log(f"Saved article urls -> {os.getcwd()}/output/article_urls.txt")
